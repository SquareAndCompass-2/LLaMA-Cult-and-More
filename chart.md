# Keeping Track of Affordable LLMs

This repository is dedicated to organizing affordable but powerful language models (LLMs). The repository providing valuable insights into the latest models, including number of parameters, fine-tuning datasets and techniques, and hardware specifications. With this repository, you can quickly and easily access all the vital information you need for your affordable LLM needs.


## Base Models
- EleutherAI: GPT-J, GPT-NEO, GPT-NEOX, Pythia / Dolly
- huggingface BigScience: BLOOM / BELLE
- Meta: OPT, LLaMA / Alpaca, Vicuna
- Microsoft: DeepSpeed Chat
- ColossalAI: ColossalChat
- Cerebras: Cerebras-GPT
- Google: BERT, T5, Flan, PaLM, LaMDA
- DeepMind: Chinchilla, Gopher, Sparrow
- Anthropic: Claude
- OpenAI: GPT-1, GPT-2, GPT-3, ChatGPT, GPT-4


## Model Spec

|project|base model|data|finetune|hardware/Cost|
|-------|-------------|-----|---------|----------|
|Stanford/Alpaca|LLaMA-7B|52K instruction-followling dataset, generate in **self-instruct** style using text-davinci-003|SFT|3 hours on 8 80GB A100s, $500(data) + $100(training)|
|NLPCloud/instruct-gpt-j|GPT-J-6B|52K Alpaca|SFT|fp16 model deploy well on 16GB Tesla T4|
|LianjiaTech/BELLE|BLOOMZ-7B1-mt|2M chinese data generated in a Alpaca way|SFT|8-bit **GPTQ** quantization using 12GB GPU|
|LianjiaTech/BELLE|LLaMA-7B| same | SFT |4-bit **ggml** quantization work well on M1 chip Mac|
|Alpaca-**LoRA**|LLaMA-7B|52K Alpaca; update to MSFT LLaMA-GPT4 dataset |SFT with LoRA|hours on a single RTX 4090(24GB)|
|Databricks/Dolly-v1-6B|GPT-J-6B|52K Alpaca|SFT|
|Databricks/Dolly-v2-12B|Pythia-12b|databricks-dolly-15k **generated by Databricks employees** in capability domains from the InstructGPT paper|SFT|about 3.5 hours on 8 V100s with fp16 to complete 1 epoch|
|GPT4All|LLaMA-7B|~800k GPT-3.5-Turbo Generations|SFT with LoRA|
|HIT&HFL/**Chinese-LLaMA-Alpaca**|LLaMA-7B/13B| ahout 2M chinese and english dataset |add **20K chinese sentencepiece tokens** to vocab to improve chinese decoding effciency; using DeepSpeed Zero-2| pretrain on 20GB general chinese corpus on 16 A100s; SFT with LoRA on 16 A100s|
|THUDM/ChatGLM-6B|
|**LLaMA-Adaptor**|LLaMA-7B|52K Alpaca|SFT with LLaMA-Adaptor| reduce 3 hours to 1 hour, 1.2M instead of 7B |
|FastChat/**Vicuna**|LLaMA-7B/13B|70K user-shared conversations gathered from ShareGPT.com|SFT, 40x larger dataset and 4x sequence length |4/8 A100s, $140/300 for training, **Impressing GPT-4 with ~90% ChatGPT Quality**|
|BAIR/Koala|LLaMA-13B|Around 60K dialogues shared by users on ShareGPT; Human ChatGPT Comparison Corpus (HC3), Open Source Data...|SFT with JAX/Flax | 2 epochs in 6 hours using 8 A100s, beat ChatGPT on 180 real user queries|
|Baize|LLaMA-7B/13B/30B|100k dialogs generated by letting ChatGPT chat with itself; QA and healthcare dataset |SFT with LoRA| run on A100(80GB)s|
|Firefly|bloom-1b4/2b6-zh|1.1M instruction dataset build from 23 chinese NLP tasks, BELLE-0.5M-cn |reduce vocab from 25w to 4.6w, SFT|
|Arxiv Chat| | | build on ChatGPT(QA), LangChain(main logic) and h2oai(UI) |
|huggingface/StackLLaMA|LLaMA-7B|Stack Exchange dataset(10M<N<100M)|SFT + RLHF|(2+8)*7B=70GB, 80GB A100 works fine, LoRA/PEFT makes 50-60B model works on a single A100 possible |
|MSFT/LLaMA-GPT4|LLaMA-7B|52K Alpaca prompt input using GPT-4| SFT, RM|
|MSFT/DeepSpeed Chat| | | support SFT, RM, RLHF | [Efficiency and Affordability](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat) |
|ColossalAI/ColossalChat| | | support SFT, RM, RLHF | [quick preview](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat#quick-preview)|


## References
[Self-Instruct: Aligning Language Model with Self Generated Instructions, 2022/12, EDUs and AllenAI]
[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, 2022/10, IST-DASLab]
[ggml: https://github.com/ggerganov/llama.cpp, 2023/03]
[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS, 2021/06, MSFT]
[PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods https://github.com/huggingface/peft]
[LLaMA-Adapter: LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention, 2023/03, Shanghai AI Lab]


## Fine-tune Stages
- SFT: Raw, LoRA, PEFT; Chinese Vocab Fixing; Instruction Dataset generated using ChatGPT/GPT4, Human labeled dataset like databricks-dolly-15k;
- RM: GPT-4 assign scores using its judging quality ability; Open Source Datasets;
- RLHF: DeepSpeedChat/ColossalChat;

![](https://openaicom.imgix.net/cf717bdb-0c8c-428a-b82b-3c3add87a600/ChatGPT_Diagram.svg?fm=auto&auto=compress,format&fit=min&w=1919&h=1138)
